---
title: "Sentiment Analysis of US Election Tweets"
author: "Clinton Roy, Mrunal Limaye, Nikita Kirane"
date: "2/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Sentiment Analysis of US Election Tweets

### Project Description:
* Sentiment analysis, also refers as opinion mining, is a sub machine learning task where we want to determine which is the general sentiment of a given document. Using machine learning techniques and natural language processing we can extract the subjective information of a document and try to classify it according to its polarity such as positive, neutral or negative.
* Twitter is an American microblogging and social networking service on which users post and interact with messages known as    "tweets". Users can post, like, and retweet tweet. Twitter proves to be an amazing platform to understand people's sentiments and opinions about anything going on in the world.
* In our project we have attempted to perform sentiment analysis of tweets related to US Elections 2020. In this we extracted tweets like #USElections2020, #Trump, #VoteforAmerica etc just to name a few.
* Our goal was to find out the overall response from the tweets related to the political sensation "US Elections 2020".

### Data:
* For data collection we used the twitter API. Following are some APIs:
GET lists/list
GET lists/members
GET lists/memberships
GET lists/ownerships

### Steps:
* Data Collection
* Data Wrangling
* Tokenization
* Stop words and number removal
* Creating bigrams
* Document term matrix
* Sentiment Analysis
```{r,message=FALSE,warning=FALSE}
library(tidytext)
library(stringr)
library(tidyverse)
library(knitr)
library(corpus)
library(tm)
library(tmap)
library(wordcloud) 	
library(sentimentr) 
library(widyr)
library(dplyr)
library(plotly)
library(syuzhet)
library(dplyr) 
library(ggplot2) 
library(devtools)
library(sentimentr)
library(kableExtra)
```

## Including Plots

You can also embed plots, for example:

```{r,message=FALSE,warning=FALSE}
rm(list = ls())
df= read_csv("mrunal.csv")
df1<-df
```

## Creating Dataframe:
```{r,message=FALSE,warning=FALSE}
rows <- c("User_ID", "Screen_Name", "Source", "Twitter_Text", "Text_Width", "Favourites_Count","Retweet_Count","Hashtags","Followers_Count","Friends_Count", "Description", "Verified_Profile")
df1<- data.frame(df$user_id,
                 df$screen_name, 
                 df$source, 
                 df$text, 
                 df$display_text_width, 
                 df$favourites_count, 
                 df$retweet_count, 
                 df$hashtags, 
                 df$followers_count, 
                 df$friends_count,
                 df$description, 
                 df$verified)

names(df1)<-rows
head(df1$Twitter_Text,5)
```
## Data Cleaning:
* To conduct sentiment analysis we had to remove the URLs from the tweets.
```{r,message=FALSE,warning=FALSE}
df1$Twitter_Text <- gsub("?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)()","",df1$Twitter_Text)
df1$Twitter_Text<-gsub("@","",df1$Twitter_Text)
head(df1$Twitter_Text,5)
```
* Remove numerical data
```{r,message=FALSE,warning=FALSE}
df1$Twitter_Text=str_replace_all(df1$Twitter_Text, "[^0-9A-Za-z///']", " ")
head(df1$Twitter_Text)
```
## Tokenizing:
```{r,message=FALSE,warning=FALSE}
tokens <- df1 %>%  
  unnest_tokens(output = word, input = "Twitter_Text") 
head(tokens)
```

```{r,message=FALSE,warning=FALSE}
tokens %>%  
  count(word,sort = TRUE)
```

```{r,message=FALSE,warning=FALSE}
sw = get_stopwords() 
sw
```

```{r,message=FALSE,warning=FALSE}
cleaned_tokens <- tokens %>%  
  filter(!word %in% sw$word)
```

```{r,message=FALSE,warning=FALSE}
nums <- cleaned_tokens %>%   
  filter(str_detect(word, "^[0-9]")) %>%   
  select(word) %>% unique() 
head(nums)
```
## Histogram of cleaned tokens:
```{r,message=FALSE,warning=FALSE}
cleaned_tokens <- cleaned_tokens %>%   
  filter(!word %in% nums$word)
cleaned_tokens %>%  
  count(word, sort = T) %>%  
  rename(word_freq = n) %>%  
  ggplot(aes(x=word_freq)) +  geom_histogram(aes(y=..count..), color="black", fill="blue", alpha=0.3) +     scale_x_continuous(breaks=c(0:5,10,100,500,10e3), trans="log1p", expand=c(0,0)) +     scale_y_continuous(breaks=c(0,100,1000,5e3,10e3,5e4,10e4,4e4), expand=c(0,0)) +  
  theme_bw()
```
## Finding rare words:
```{r,message=FALSE,warning=FALSE}
rare <- cleaned_tokens %>%   
  count(word) %>%  
  filter(n<5000) %>%  
  select(word) %>% 
  unique() 
head(rare)
```
## Word Cloud:
* Evaluating sentiments at a glance:
```{r,message=FALSE,warning=FALSE}
pal <- brewer.pal(8,"Dark2") # plot the 100 most common words 
cleaned_tokens %>%   count(word) %>%  
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))
```
## Sentiment Analysis on sentiment lexicon in tidytext:

* The nrc lexicon: word and their sentiment category
* The bing lexicon: word and their polarity (negative or positive)
* The ann lexicon: word and their numeric sentiment score
```{r,message=FALSE,warning=FALSE}
head(get_sentiments("nrc"))
```

```{r,message=FALSE,warning=FALSE}
head(get_sentiments("afinn"))
```

```{r,message=FALSE,warning=FALSE}
sent_reviews = cleaned_tokens %>%   
  left_join(get_sentiments("nrc")) %>%  
  rename(nrc = sentiment) %>%  
  left_join(get_sentiments("bing")) %>%  
  rename(bing = sentiment) %>%  
  left_join(get_sentiments("afinn")) %>%  
  rename(afinn = value)
head(sent_reviews)
```

```{r,message=FALSE,warning=FALSE}
bing_word_counts <- sent_reviews %>%  
  filter(!is.na(bing)) %>%  
  count(word, bing, sort = TRUE) 
head(bing_word_counts)
```
### Contribution to Sentiments:
*To visualize the contribution of sentiment(positive/negative) as per the words in reviews:
```{r,message=FALSE,warning=FALSE}
bing_word_counts %>%  
  filter(n > 500) %>%  
  mutate(n = ifelse(bing == "negative", -n, n)) %>%  
  mutate(word = reorder(word, n)) %>%  
  ggplot(aes(word, n, fill = bing)) +  
  geom_col() +  
  coord_flip() +  
  labs(y = "Contribution to sentiment")

```

```{r,message=FALSE,warning=FALSE}
sentiment("i support trump")
```
### Creating Bigrams:
```{r,message=FALSE,warning=FALSE}
bigrams <- df1 %>%  unnest_tokens(bigram, Twitter_Text, token = "ngrams", n = 2) 
head(bigrams) %>% select(bigram) %>% count(bigram, sort = TRUE)


bigrams_separated <- bigrams %>%  
  separate(bigram, c("word1", "word2"), sep = " ") 

bigrams_filtered <- bigrams_separated %>%  
 filter(!word1 %in% stop_words$word) %>%  
  filter(!word2 %in% stop_words$word)
 #new bigram counts: 
head(bigrams_filtered) %>% 
  count(word1, word2, sort = TRUE)

```

```{r,message=FALSE,warning=FALSE}
uncommon <- cleaned_tokens %>%   
  count(word) %>%  
  filter(n<500) %>% #remove rare words      
  select(word) %>% unique()

head(cleaned_tokens)
```

```{r,message=FALSE,warning=FALSE}
df1$Twitter_Text %>% 
  get_sentences() %>% 
  sentiment() %>% 
  mutate(characters = nchar(stripWhitespace(df1$Twitter_Text))) %>% 
  filter(df1$Twitter_Text >1 ) -> bounded_sentences 
summary(bounded_sentences$sentiment)
```

```{r,message=FALSE,warning=FALSE}
bounded_sentences %>% filter(between(sentiment,-1,1)) ->  bounded_sentences
d <- with(density(bounded_sentences$sentiment), data.frame(x, y))
```
## Sentiment Graph:
```{r,message=FALSE,warning=FALSE}
ggplot(d, aes(x = x, y = y)) +
  geom_line() +
  geom_area(mapping = aes(x = ifelse(x >=0 & x<=1 , x, 0)), fill = "blue") +
  geom_area(mapping = aes(x = ifelse(x <=0 & x>=-1 , x, 0)), fill = "red") +
  scale_y_continuous(limits = c(0,7.5)) +
  theme_minimal(base_size = 16) +
  labs(x = "Sentiment", 
       y = "", 
       title = "Sentiment Graph") +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.y=element_blank()) -> gg

plot(gg)
```

```{r,message=FALSE,warning=FALSE}

emotions <- get_nrc_sentiment(df1$Twitter_Text)
emo_bar = colSums(emotions)
emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])
p <- plot_ly(emo_sum, x=~emotion, y=~count, type="bar", color=~emotion) %>%
  layout(xaxis=list(title=""), showlegend=FALSE,
         title="Emotion types for US Elections 2020")
p
```

```{r,message=FALSE,warning=FALSE}
wordcloud_tweet = c(
  paste(df1$Twitter_Text[emotions$anger > 0], collapse=" "),
  paste(df1$Twitter_Text[emotions$anticipation > 0], collapse=" "),
  paste(df1$Twitter_Text[emotions$disgust > 0], collapse=" "),
  paste(df1$Twitter_Text[emotions$fear > 0], collapse=" "),
  paste(df1$Twitter_Text[emotions$joy > 0], collapse=" "),
  paste(df1$Twitter_Text[emotions$sadness > 0], collapse=" "),
  paste(df1$Twitter_Text[emotions$surprise > 0], collapse=" "),
  paste(df1$Twitter_Text[emotions$trust > 0], collapse=" ")
)

# create corpus
corpus = Corpus(VectorSource(wordcloud_tweet))

# remove punctuation, convert every word in lower case and remove stop words

corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("english")))
corpus = tm_map(corpus, stemDocument)

# create document term matrix

tdm = TermDocumentMatrix(corpus)

# convert as matrix
tdm = as.matrix(tdm)
tdmnew <- tdm[nchar(rownames(tdm)) < 11,]

# column name binding
colnames(tdm) = c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust')
colnames(tdmnew) <- colnames(tdm)
comparison.cloud(tdmnew, random.order=FALSE,
                 colors = c("#00B2FF", "red", "#FF0099", "#6600CC", "green", "orange", "blue", "brown"),
                 title.size=1, max.words=250, scale=c(2.5, 0.4),rot.per=0.4)

```

```{r,message=FALSE,warning=FALSE}
cleaned_tokens %>%  
  count(word,sort=TRUE) %>%
  top_n(10) %>% 
  mutate(word = reorder(word,n)) %>% 
  ggplot(aes(x=word,y=n)) +
  geom_col() +
  xlab(NULL) + 
  coord_flip() +
  theme_classic() +
  labs(x="Count",
       y= "Unique Words",
       title="Unique word count found in all tweets")
```

```{r,message=FALSE,warning=FALSE}

sentiments_df <- sentiment_attributes(df1$Twitter_Text)
new_2 <- get_sentences(df1$Twitter_Text)

tweet_sentiment<-sentiment_by(new_2, averaging.function = average_weighted_mixed_sentiment)
#visualization of sentiments

sentiment_graph = plot_ly(x=tweet_sentiment$word_count,y=tweet_sentiment$ave_sentiment,mode="markers",colors =c("red","yellow"),size=abs(tweet_sentiment$ave_sentiment)/3 , color=ifelse(tweet_sentiment$ave_sentiment>0,"Positive","Negative") ) %>% 
#Change hover mode in the layout argument 
layout( hovermode="closest",title="Sentiment analysis by Tweet",xaxis= list(title = "Number of words per Tweet",size=18),yaxis = list(title = "Sentiments by Tweet",size=18))
# show the graph
sentiment_graph
```

```{r,message=FALSE,warning=FALSE,fig.width=10,fig.height=11}
table_tweet <- data.frame(head(sort(table(df1$Source),decreasing=T),20))
ggplot(table_tweet, aes(x=Var1, y=Freq)) +
  geom_segment( aes(x=Var1, xend=Var1, y=0, yend=Freq)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Tweets by device/source", x="Device/Source",y="Frequency")+
  geom_point( size=5, color="red", fill=alpha("orange", 0.3), alpha=0.7, shape=21, stroke=2)
```

